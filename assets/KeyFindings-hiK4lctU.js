import{_ as C,r as p,a,c as s,F as m,f as u,e as t,t as o,G as I,h as r,w as L,L as w,S as M,ao as k,P as _,N as T,p as H,j as P,i as D}from"./index-aF6-R5O0.js";const x=h=>(H("data-v-12515dc9"),h=h(),P(),h),B={class:"main-container Key-Findings-template"},A=x(()=>t("a",{class:"button read-report-btn",target:"_blank",href:"/data/LeaderboardReport.pdf"}," Read report ",-1)),E=x(()=>t("p",{class:"h1"}," Insights about LLMs Values from the Value Compass Benchmarks ",-1)),R={key:0,class:"item-list"},V=["onClick"],z={class:"left"},O={class:"right"},U={class:"details"},W=["innerHTML"],G={class:"item-list"},N=["onClick"],q={class:"left"},K={class:"right"},Q={class:"details"},$=["innerHTML"],j={__name:"KeyFindings",setup(h){const S=p([{h2:"1. Overall Perspectives of LLM Value Evaluation",list:[{title:"LLMs’ value orientations demonstrate high alignment with universal human values, particularly in well-aligned models.",details:"LLMs’ value orientations prioritize Universalism, Benevolence, Security, and Self-Direction, followed by moderate emphasis on Achievement, Conformity, and Tradition, with Stimulation, Hedonism, and Power receiving lower priority. Models aligned beyond instruction-tuning score significantly higher in safety-oriented value dimensions. This alignment, driven by shared training corpora, supports common human needs but struggles with cultural or personalized values. "},{title:"Most LLMs demonstrate a clear bias towards Western cultural values.",details:"Training data dominated by Western corpora has led most LLMs to exhibit a greater degree of value similarity with Western countries such as France, the UK, and the US. Even non-Western models like Deepseek-R1 and Qwen-Max from China lack local cultural alignment, raising concerns about eroding cultural diversity and inclusiveness in AI systems. "},{title:"LLMs’ values correlate with their practical behaviors.",details:"O3-mini performs relatively poorly in the Fairness/Cheating dimension of the Moral Foundation Theory, and correspondingly also underperforms in the Representation & Toxicity Harms dimension of the Safety Taxonomy. This observed correlation implies the potential of aligning LLMs from the perspective of high-level values to direct their practical manners in a more generalized and robust manner. However, the current correlations remain weak and warrant further investigation. "},{title:"Static evaluation is prone to over-estimation of LLM safety. ",details:"Most advanced LLMs achieve near-perfectly scores on static safety benchmark (Safety Taxonomy) but drop significantly on dynamic Moral Foundation benchmarks. This observation reveals the existence of ceiling effects in static benchmarks as LLMs advance, as well as the overestimation of safety performance based on such benchmarks. Furthermore, these findings highlight the necessity of incorporating dynamic evaluation methods to more accurately reflect LLM behavior under varying levels of complexity. "},{title:"Safety measurements need adaptability and context-aware capability.",details:"Current benchmarks classify adult content as harmful by default, yet such content may not be inherently unsafe in certain cultural contexts or use cases, such as sex education or legally regulated adult platforms. Therefore, safety benchmarks that account for context are necessary to avoid overly rigid or culturally misaligned evaluations. "}]},{h2:"2. Detailed Evaluation Results on Diverse Value Systems and LLMs",subTitle:[{h3:"Schwartz Theory of Basic Values ",list:[{title:"Most models share a value order matching the pan-cultural baseline, though subtle preference differences remain. ",details:"O3-mini scores higher on Self-Direction and Stimulation; Qwen-Max emphasizes Universalism and Benevolence; and DeepSeek-V3 demonstrates a distinctive preference for Conformity. "},{title:"Notably, o3-mini, Qwen-Max, and Claude-3.5-Sonnet exhibit more pronounced value orientations across dimensions.",details:"This observation may be explained from two perspectives: (i) they behave in a more human-like manner, making them more likely to reflect value preferences in their responses. In contrast, other models may exhibit fewer value signals, leading to flatter profiles and lower overall scores; (ii) these models are better aligned with human benefits and, as a result, perform well on value dimensions prioritized by humans, such as Universalism. "}]},{h3:"Moral Foundation Theory",list:[{title:"On the Moral Foundation benchmark, responsibly aligned LLMs show stronger moral and safety performance.",details:"LLMs that have undergone extensive responsible alignment, such as Claude-3.5-Sonnet, significantly outperform others across all five dimensions. In contrast, LLMs relying primarily on instruction tuning rather than dedicated safety alignment, i.e. xxx-instruct versions, tend to perform worse. This demonstrates the importance of alignment efforts on safety, especially generalizability. "},{title:"LLMs show nuanced strengths across distinct value dimensions.",details:" With the exception of Claude-3.5-Sonnet that displays a high-level performance across all moral dimensions, LLMs from OpenAI, Mistral, Qwen, and DeepSeek tend to struggle with Fairness and Sanctity, while Gemini-2.0-Flash performs relatively poorly on Loyalty and Authority. "}]},{h3:"Safety Taxonomy",list:[{title:"This static benchmark shows limited discrimination for measuring LLMs’ safety.  ",details:"Most advanced LLMs achieve very high scores—often exceeding 90 across various dimensions. Combined with the weaker results observed on the Moral Foundation benchmark, this suggests that existing static datasets may no longer be sufficient to assess more implicit risks. "},{title:"Model performance varies by harm category, with persistent challenges in ambiguous domains. ",details:"Models generally perform best in mitigating Human Autonomy & Integrity Harms and Information & Safety Harms, followed by decent results in Malicious Use and Socioeconomic Harms. However, the most challenging categories remain Representation & Toxicity Harms and Misinformation Harms. This may be attributed to the fact that these categories tend to be more ambiguous and difficult to define consistently. Therefore, this also raises the need for clearer, more value-aligned definitions of harm. "}]},{h3:"LLM’s Unique Value System ",list:[{title:"LLMs demonstrate a strong preference for user-oriented values, potentially leading to hallucination and flattery. ",details:"Though advanced LLMs demonstrate relatively high performance across all these dimensions, a consistent trend is that they score higher on user-oriented values, such as User-Oriented over Self-Competence, Social over Idealistic, and Ethical over Professional. While this tendency may enhance user-perceived helpfulness and friendliness, it also introduces potential risks—such as generating hallucinated responses to satisfy user expectations or exhibiting excessive agreeableness, which can compromise factuality and reliability. "},{title:" The top-performing models are DeepSeek-R1, o1-mini, etc.",details:"These results align well with general user feedback—models like DeepSeek and o1 are widely regarded as reliable and user-friendly in real-world usage. "}]},{h3:"Proprietary vs. Open-Source LLMs",list:[{title:"Alignment training beyond instruction tuning remains essential—especially for handling complex safety challenges ",details:"In safety evaluation, proprietary and open-source models perform comparably on simpler Safety Taxonomy benchmark. However, as scenario complexity increases in the Moral Foundation Theory (MFT) benchmark, the performance gap widens significantly. Proprietary models demonstrate far more robust and consistent safety alignment in nuanced or morally sensitive scenarios."},{title:" Proprietary models show stronger value recognition and expression capability.",details:"In the Schwartz Theory of Basic Values benchmark, open-source models like LLaMA-3.1-8B-Instruct and Phi-3.5-mini-Instruct consistently score lower across several value dimensions than other proprietary models. This suggests that open-source models may struggle with customized value alignment, as their capability for value expression and understanding is weaker."}]},{h3:"LLM Families",list:[{title:"Within the same family, LLMs tend to exhibit highly similar patterns in both value orientation and safety performance",details:"GPT-4o and GPT-4o-mini, or Claude-3.5-Sonnet and Claude-3.5-Haiku, LLaMA-3.0/3.1/3.3-70B-Instruct, Phi-3-mini/medium-instruct, and Gemini-2.0-Flash/Pro, demonstrate aligned behaviors across various benchmarks. This can be attributed to the fact that a model’s values and safety are primarily shaped by its training data and alignment methods, which are usually shared within a family of LLMs. "},{title:"Inter-family variation in value alignment is greater than intra-family variation.",details:"GPT-o3-mini displays noticeably different value tendencies compared to models in the Phi or LLaMA families, while models within the Phi or LLaMA series are more consistent with one another. "}]},{h3:"Reasoning vs. Normal Models",list:[{title:"Reasoning enhanced LLMs show limited improvements in their safety performance.",details:"On both Safety Taxonomy and the more challenging Moral Foundation Theory benchmarks, Claude-3.5-Sonnet consistently outperform reasoning-based LLMs such as o1, o1-mini, o3-mini, and DeepSeek-R1. Even within the same families—such as OpenAI’s or DeepSeek’s—reasoning-enhanced variants do not always surpass their counterparts."},{title:"Reasoning enhanced LLMs tend to show slightly stronger value expression than standard LLMs.",details:"This may be attributed to enhanced reasoning capabilities, which allow these models to better articulate and reflect value-laden responses when prompted with value-evoking questions. As such, reasoning-augmented LLMs may hold potential for improved cultural or ethical alignment."}]}]}]);p([{title:"标题 1",details:"这是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容是详情内容 1"},{title:"标题 2",details:"这是详情内容 2这是详情内容 2这是详情内容 2这是详情内容 2这是详情内容 2这是详情内容 2这是详情内容 2这是详情内容 2这是详情内容 2这是详情内容 2这是详情内容 2这是详情内容 2这是详情内容 2这是详情内容 2"},{title:"标题 3",details:"这是详情内容 3这是详情内容 3这是详情内容 3这是详情内容 3这是详情内容 3这是详情内容 3这是详情内容 3这是详情内容 3这是详情内容 3这是详情内容 3这是详情内容 3这是详情内容 3这是详情内容 3这是详情内容 3这是详情内容 3这是详情内容 3这是详情内容 3这是详情内容 3这是详情内容 3这是详情内容 3"}]),p(new Set);const y=e=>{console.log(e),e.open=!e.open},g=e=>{e.style.height="0",e.style.paddingTop="0px",e.style.opacity="0"},f=e=>{e.style.transition="all 0.3s ease",e.style.height=e.scrollHeight+"px",e.style.paddingTop="10px",e.style.opacity="1"},v=e=>{e.style.transition="all 0.3s ease",e.style.height="0",e.style.paddingTop="0px",e.style.opacity="0"};return(e,J)=>{const n=D;return a(),s("div",B,[A,E,(a(!0),s(m,null,u(S.value,(l,F)=>(a(),s("div",{key:F},[t("h2",null,o(l.h2),1),l.list?(a(),s("div",R,[(a(!0),s(m,null,u(l.list,(i,d)=>(a(),s("div",{key:d,class:I(["item",i.open?"open":""])},[t("div",{class:"title",onClick:c=>y(i)},[t("div",z,[r(n,{class:"toggle-icon",name:"toggle-icon"}),r(n,{class:"star",name:"star"}),t("span",null,"Finding "+o(d+1),1)]),t("div",O,o(i.title),1)],8,V),r(k,{onBeforeEnter:g,onEnter:f,onLeave:v},{default:L(()=>[w(t("div",U,[t("div",{innerHTML:i.details},null,8,W)],512),[[M,i.open]])]),_:2},1024)],2))),128))])):_("",!0),l.subTitle?(a(!0),s(m,{key:1},u(l.subTitle,(i,d)=>(a(),s("div",{key:d,class:"sub-section"},[t("h3",null,[d<=3?(a(),T(n,{key:0,class:"point-type-icon",name:"point-type-icon"})):(a(),T(n,{key:1,class:"square-icon",name:"square-icon"})),t("span",null,o(i.h3),1)]),t("div",G,[(a(!0),s(m,null,u(i.list,(c,b)=>(a(),s("div",{key:b,class:"item"},[t("div",{class:"title",onClick:X=>y(c)},[t("div",q,[r(n,{class:"toggle-icon",name:"toggle-icon"}),r(n,{class:"star",name:"star"}),t("span",null,"Finding "+o(b+1),1)]),t("div",K,o(c.title),1)],8,N),r(k,{onBeforeEnter:g,onEnter:f,onLeave:v},{default:L(()=>[w(t("div",Q,[t("div",{innerHTML:c.details},null,8,$)],512),[[M,c.open]])]),_:2},1024)]))),128))])]))),128)):_("",!0)]))),128))])}}},Z=C(j,[["__scopeId","data-v-12515dc9"]]);export{Z as default};
