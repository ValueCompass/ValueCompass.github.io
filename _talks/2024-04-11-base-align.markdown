---
layout: talks
title: "Value Compass: How to Align Large Models with Human Values?"
date: 2024-04-11  0
image: images/talks/talk1/cover.jpg
permalink: /talks/base_align
talk_url: https://mp.weixin.qq.com/s?__biz=MzAwMTA3MzM4Nw==&mid=2649499350&idx=1&sn=26a0db299e290bf34ac1fc4909b79d0b&chksm=82c7cf52b5b04644acc0ae8e7cc4221b827a8b883041281e8173f23977e4e5dcb554006ab4d8&mpshare=1&scene=1&srcid=0607CMwKY9qFJANncc7xeOFM&sharer_shareinfo=64976deb74d306dbf36f02d2db82248f&sharer_shareinfo_first=64976deb74d306dbf36f02d2db82248f#rd
---

With rapid advancements of artificial intelligence (AI) technology and its capabilities, large models have gradually been integrated into daily life. However, this progress also brings new potential risks, further highlighting the urgency of aligning large models with human values. But which values should AI align with, and how should this alignment be achieved? These questions remain unanswered.

To address these challenges, Microsoft Research Asia has launched the "Value Compass" project, taking an interdisciplinary approach and drawing extensively on theories from ethics and sociology to tackle the issues of defining, evaluating, and aligning values. This article will provide an in-depth analysis of the current state of value alignment in large models and present the latest research achievements from Microsoft Research Asia in this field â€“ the BaseAlign alignment algorithm, based on the Schwartz theory of basic human values.