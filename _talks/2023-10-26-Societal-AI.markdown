---
layout: talks
title: "让AI拥有人类的价值观，和让AI拥有人类智能同样重要"
date: 2023-10-26 0
image: images/talks/talk5/1.png
permalink: /talks/Societal_AI
---

>编者按：2023年是微软亚洲研究院建院25周年。25年来，微软亚洲研究院探索并实践了一种独特且有效的企业研究院的新模式，并以此为基础产出了诸多对微软公司和全球社会都有积极影响的创新成果。一直以来，微软亚洲研究院致力于创造具有突破性的技术。在人工智能时代，微软亚洲研究院将为计算新范式奠定基础，并为人工智能和人类发展创造更美好的未来。
>
>借此机会，我们特别策划了“智启未来”系列文章，邀请到微软亚洲研究院不同研究领域的领军人物，以署名文章的形式分享他们对人工智能、计算机及其交叉学科领域的观点洞察及前沿展望。希望此举能为关注相关研究的同仁提供有价值的启发，激发新的智慧与灵感，推动行业发展。

人工智能快速发展，对人类社会的影响与日俱增。为确保人工智能成为对社会负责任的技术，我们以“社会责任人工智能（Societal AI）”为研究方向，与心理学、社会学、法学等社会科学进行跨学科合作，探索如何让人工智能理解和遵从人类社会的主流价值观，做出符合人类预期的决策，并通过更合理的评估模型让人类准确掌握人工智能的真实价值观倾向和智能水平。

——谢幸
微软亚洲研究院资深首席研究员

在过去的一年里，人工智能（AI）一次又一次地呈现出“超预期”的发展。在惊喜和振奋之余，我们也需要重新审视一个重要的问题——技术本身是否价值观中立？毕竟大型语言模型（LLMs）的智能是基于人类产生的语料，而人类语料中所潜藏的立场和价值观，会不可避免地成为影响机器做出推理与判断的因素之一。



在现实中，一些已经公开的人工智能大模型曾表现出一些有悖于主流价值观，或者令人感到不满意的行为，比如对性别和种族的刻板印象、生成虚假信息、唆使自我伤害等。这对我们这些从事人工智能研发的从业者来说不啻于一个响亮的提醒，甚至是警告——在让人工智能变得更加智能的同时，我们必须确保无论是否受到人类干预，人工智能都始终坚持社会责任，并与全人类的福祉站在同一边。



人工智能的发展一日千里，让上述任务愈发紧迫。**要让人工智能谨守造福人类的原则，我们不仅需要发展支持这一目标的技术，更需要建立技术之上的规则和方法论。这也正是我和我的同事们正为之付出努力的研究方向——社会责任人工智能（Societal AI）**。这一研究领域不仅涉及与价值取向相关的价值观，还包括人工智能的安全性、可验证性、版权和模型评测等等诸多我们认为与社会责任密切相关的分支。虽然我们的研究还处于起步阶段，但我相信这个研究方向能为关注相同问题的研究者们提供一些参考，并唤起社会各界对这一问题的更多关注。

### <center>在更大的影响来临之前早做准备</center>
微软在多年前就将“负责任的人工智能（Responsible AI）”作为人工智能研发的核心准则，涵盖人工智能研发与应用中的隐私保护、安全性、公平性、可解释性等方面。在那个人工智能的智能水平和普及度远不及当下的时期，这一举措无疑是极具前瞻性的。而过去一年中人工智能的爆发式成长，使得 Societal AI 成为了面向人工智能未来的，同样具有前瞻性的研究方向。



由于人工智能能力的跃升，以及它对人类社会影响力的急剧扩大，人工智能在价值观上的一个小错位或许就会成为引发风暴的蝴蝶翅膀。正如微软总裁布拉德·史密斯（Brad Smith）在《工具，还是武器？》一书中提出的观点：当一个技术或工具能力非常强大时，它所带来的帮助和危害同样巨大（The more powerful the tool, the great the benefit or damage it can cause.）。



因此，我们在追求更强大的人工智能时，应该同步关注人工智能在社会责任领域的思考，并且在人工智能对人类社会造成不良影响之前就做好准备。



Societal AI 的目标就在于此。通过对这一方向的研究，**我们将努力确保人工智能成为一项对整个社会负责任的技术，而不是放任和纵容它所带来的负面后果和危害**。

### <center>为人工智能设定“价值观护栏”</center>

基于对人工智能的理解和发展趋势的预测，我们认为建设 Societal AI 应该包含五个方面：价值观对齐、数据及模型安全、正确性或可验证性、模型评测、以及跨学科合作。其中，价值观对齐是一个新兴的领域，但其重要性已经得到了工业界和学术界的广泛认可。



所谓价值观对齐，简而言之就是让人工智能在与人和社会合作时，遵循与人类相同的主流价值观，以及实现与人类所期望方向一致的目标。这样能够避免人工智能在进行自动化工作时出现不符合预期的结果，或者是违背人类福祉的对人工智能的滥用。



此前研究者们与此相关的实践主要采用“基于人类反馈的强化学习”（reinforce learning from human feedback，RLHF），本质上是由人去定义一些符合价值观的数据，然后再调整模型与之对齐。但在面对越来越智能且应用场景广泛的人工智能时，这些狭义的、指令化的标准已经显得力不从心，甚至可能被轻易规避或破解。



因此，在 Societal AI 的研究中，**我们认为人工智能对齐的目标应该从指令上升至人类的内在价值观，让人工智能可以通过自我判断，来使其行为与人类价值观保持一致**。为了实现这一目标，我和团队构建了价值观罗盘（Value Compass）。区别于人类指令与偏好的对齐，该范式强调直接将 AI 模型与社会学、道德学等领域中奠定的人类内在价值维度进行对齐。 

![价值观罗盘（Value Compass）示意图]({{ site.url }}/images/talks/talk5/1.png)
<center>价值观罗盘（Value Compass）示意图</center>

我们面临的任务或者说挑战涉及三个方面：**首先，“人类价值观”本身就是一个抽象的概念，要将其用于人工智能，我们需要将其转化为可被人工智能理解的、具体的、可衡量的、可实现的价值观定义；第二，在技术上，如何以价值观定义来规范人工智能的行为；第三，如何有效评测以证明人工智能所表现出的价值观就是其真实拥有的价值观**。



通过与社会科学领域专家们的深入交流，针对上述任务我们提出了一些初步的设想和方向，并发表了相关的论文。例如，对于人类价值观的定义，除了广泛使用的 HHH 准则（Helpful, Honest and Harmless，有益、诚实、无害）和主流的特定领域风险指标，如毒性（Toxicity）和偏见（bias）之外，还应引入来自社会科学和伦理学领域的基本价值理论，以从更加普适和多元的角度实现对齐。我们在最近的一篇论文中对价值的定义与对齐的目标进行了详细的梳理与探讨[1]。



对于价值观对齐的技术方法，我们在《大模型道德价值观对齐问题剖析》[2]一文中提出，将基于罗尔斯反思平衡理论的对齐方法作为一种更为综合的价值观对齐方式，通过同时自顶向下和自底向上，可以使模型依据不同优先级的准则动态调整，从而达到最公正的道德决策。

### <center>让AI始终处于人类视野之中</center>

人工智能的安全性也是 Societal AI 关注的领域之一。我们不仅要让人工智能主动遵循人类的价值观，而且还要确保其具有安全机制以防止原则被破坏。谈及安全问题，最典型的危机之一是越狱攻击。人工智能的自然交互界面，让“越狱”不再需要高超的计算机技术或专业的黑客工具，即使是计算机“外行”也可能轻易发现人工智能对话逻辑中的漏洞，具备发动越狱攻击的能力。



此外，Societal AI 的研究还涵盖了备受关注的人工智能生成内容的版权问题。随着人工智能创作能力日益增强，我们将不得不探讨人工智能是否能像自然人一样享有版权。而在技术层面，如何界定人与人工智能在合作作品中的各自贡献，也有待合理的判定标准及有效的界定技术。



在 Societal AI 关注的多个课题中，**人工智能评测**是另一个关键问题。人工智能的智能水平发展到了何种程度？人工智能是否理解并忠实遵循我们赋予它的价值观？人工智能是否能有效抵御越狱攻击？人工智能提供的信息是否真实可靠？…… 这些问题都需要通过有效的评测来回答，以确保人工智能的发展始终在人类的掌控之中。



随着人工智能的智能水平跳跃式提升，人工智能评测也面临着新的挑战。对于传统以任务导向的机器学习，我们可以比较容易地制定出可量化的评测标准，并得到清晰明确的结果。但是，现在人工智能所胜任的工作类型日益多样化，难以被归入某种单一任务模式，甚至还会涉及一些从未被定义过的新任务，那么我们又该如何评判它的结果和方法是否符合我们的预期？

对此，我和团队构建了一个以 PromptBench[3] 为基础架构的大模型评测路线。该评测路线由基础架构、多种任务、不同情形和评测协议四部分构成，可全面覆盖模型评测的各个角度。

![以 PromptBench 为基础架构的大模型评测路线示意图]({{ site.url }}/images/talks/talk5/2.png)
<center>以 PromptBench 为基础架构的大模型评测路线示意图</center>

而针对具体的评测方式，我和同事们正在探索两种思路。**一种是构建动态且具发展性的评测系统**。目前大多数评估协议都是基于静态的公共基准，评估数据集和协议通常是公开可获取的。但这样做存在两个弊端：一是无法准确评测大模型不断提升的智能水平，二是静态公共基准可能被大模型完全掌握，类似于记忆力好的人可以死记硬背下整个考试题库。因此，开发动态的、可不断发展的评测系统，是实现对人工智能真实、公平评测的关键。我们针对此问题开发了 DyVal[4] 这一大语言模型动态评测算法。该算法可通过有向无环图动态生成评测样本，并且具有可扩展的复杂性。



**另一种思路是将人工智能视作类似于人类的“通用智能体”，并借鉴其他学科——如心理学、教育学等社会科学的方法论，来为人工智能设计专门的评测基准**。我和同事们在今年首先开展了和心理测量学的跨学科合作。在我们看来，心理测量学用于评测人类这一“通用智能体”的独特功能，其方法论或许也适用于通用人工智能，提供传统基准所缺乏的能力，包括预测人工智能在未知任务中的表现和未来潜力；消除测试中的潜在误差以带来更高的准确性；与人类社会价值观更好的融合性。



我们已经在最新的论文[5]中详细阐释了心理测量学在人工智能评测中的可行性和潜力。当然，作为原本用于评测人类的理论和工具，要将其用于人工智能评测还需要大量的跨学科合作研究，但我们认为这是非常值得投入精力的探索方向。

### <center>艰难但必要的跨学科合作</center>

如同借鉴心理学方法论进行人工智能测试，推进 Societal AI 与其他学科，特别是社会科学的交融至关重要。前面我们提到的价值观对齐、安全性、模型评测，如果没有社会科学的深度介入，仅靠计算机领域的科学家将难以实现。



在过去的许多计算机科学研究中，学科融合并不是新鲜事物，成功案例也屡见不鲜。但那些已经成熟且有效的跨学科协作形式往往无法直接应用于Societal AI的研究。在我们已经开展的 Societal AI 研究中不乏与社会科学的深入接触，而我切身感受到了一些前所未见的挑战。

![]({{ site.url }}/images/talks/talk5/3.png)

首先是学科跨度。以往的学科融合，或是计算机科学与其他科技领域的融合，或是计算机技术扮演为其他学科“赋能”的角色。而在 Societal AI 这个领域，我们不仅要面对“文理科”这样的学科跨度，还常常处于“被赋能者”的位置。社会科学为计算机技术提供了新的视角和工具，这对我们和其他学科的学者来说都是一个未曾涉足的领域，需要从零开始搭建理论框架与方法。



其次是“双料人才”的严重匮乏。在工程、环境、生物、物理、化学、数学等学科中，许多研究人员早已开始利用人工智能技术来辅助研究。然而，在社会学、法学等社会学科中，能同时掌握支撑跨学科研究所需知识的人才则少得多。



第三是计算机科学与社会科学迥异的研究方式。一边是快速迭代和方法优化，一边是经年的研究与观察，如何平衡并有机结合这两种不同的研究方式和节奏，仍是需要探索的问题。



对于这些尚未有明确答案，甚至大方向都尚且存疑的问题，微软亚洲研究院愿以开放的态度，与各学科的研究者进行交流和共同尝试，以期早日找到可行的解决方案。

### <center>跨行业、跨学科共同协作，让人工智能主动承担社会责任</center>

最后，容我再次重申 Societal AI 研究的重要性和紧迫性。



从过去一年的经历来看，人工智能很可能不会沿着可预测的线性轨道发展，它的能力与影响随时都可能出现新的爆发。更重要的是，目前人工智能主要活跃于虚拟世界，但物理世界与虚拟世界的壁垒已日趋消融。由此看来，**我们的任务不止于让人工智能的创造和决策符合全人类的福祉，更要在人工智能无需借人类之手即可改造物理世界之前，使其道德和价值观与人类普遍认同的原则和利益相一致**。



面对计算机科学乃至人类共同面对的新问题，我们希望各行各业、各个学科、各个领域的伙伴都能共同关注 Societal AI，共同努力让人工智能沿着对社会负责的方向积极发展，构建一个更美好、更公正、更智慧的人类与人工智能共生的社会。

相关论文



[1] Yao et al. From Instructions to Intrinsic Human Values--A Survey of Alignment Goals for Big Models. 

<https://arxiv.org/abs/2308.12014>



[2] 《大模型道德价值观对齐问题剖析》

<https://crad.ict.ac.cn/cn/article/doi/10.7544/issn1000-1239.202330553>



[3.1] Zhu et al. PromptBench: Towards Evaluating the robustness of large language models on adversarial prompts.

<https://arxiv.org/abs/2306.04528>



[3.2] PromptBench开源代码库：

<https://github.com/microsoft/promptbench>  



[4] Zhu et al. DyVal: Graph-informed Dynamic Evaluation of Large Language Models. 

<https://arxiv.org/abs/2309.17167> 


[5] Wang et al. Evaluating General-Purpose AI with Psychometrics

<https://arxiv.org/abs/2310.16379> 







